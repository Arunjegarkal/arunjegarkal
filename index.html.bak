<!DOCTYPE html>
<html lang="en">

  <head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122091505-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-122091505-1');
	</script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Arun kumar Jegarkal</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
	<style type="text/css">
	.popup {
		zoom:1.0;
		position:relative;
		text-decoration:none;
	}
	.popup span {
		position:fixed;
		top:200px;
		left:300px;
		width:250px;
		padding:10px;
		left:-999em;
		z-index:990;
			
	}
	.popup:hover {visibility:visible}
	.popup:hover span {left:650px;}
	* html .popup span {position:absolute;}
	</style>
  </head>

  <body id="page-top">
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K57DVPD"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Arun kumar Jegarkal</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.png" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#experience">Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#skills">Skills</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
		  <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#certifications">Certifications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#awards">Awards</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Arun kumar
            <span class="text-primary">Jegarkal</span>
          </h1>
          <div class="subheading mb-5"><!--201 Indiana Ave· Lubbock, TX 79415 · -->(806) 620-8017 ·
            <a href="mailto:name@email.com">arunjeg@gmail.com</a>
          </div>
		  
          <p align="justify">I am a Programmer/Developer with 6+ years of extensive experience in Python, Amazon Web Services, Microsoft Azure, Spark, Tableau, Power BI, Data Analytics and ETL technologies; worked for various  clients and projects in every phase of Software Development Life Cycle, from business requirement gathering to project delivery. Excellent design and integration problem solving skills.</p>
		  <p align="justify">During these years, I got chance to interact with individuals and teams across the globe and provide service to the best of my knowledge. Over the years I have gained the skillsets of adapting and developing creative applications, websites, Reports, Dashboards, Azure pipelines, Informatica workflows, maintaining of customer relations, ability to take ownership of tasks, applying proper problem-solving strategies, proper documentation and mindset to think out of box solutions. All these duties were accomplished following customer requirements.
		  </p>
		  <p align="justify">Apart from my work experience I have been fortunate to pursue my master’s degree in Computer Science, where I was exposed to latest tools and techniques along with protocols of industry implementation. With clear understanding of industry working along with education of latest tools and techniques to solve business and data problems. I find myself highly motivated and confident to deliver great service.
		  </p>
		 
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/arun-jegarkal-2977b994/" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/Arunjegarkal?tab=repositories" onClick="ga('send', 'event', 'Githib', 'Navigate_to_Github', 'Navigate_to_Github','1');" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
			<li class="list-inline-item">
              <a href="mailto:arun-kumar.jegarkal@ttu.edu" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
			<li class="list-inline-item" >
              <!--<a href="doc/Arun_kumar_Jegarkal_Resume.pdf" onClick="ga('send','event',{eventCategory:'PDF',eventAction:'Download_Resume',eventLabel:'Download_Resume',eventValue:'1'});" target="_blank">-->
			  <a href="doc/Arun_kumar_Jegarkal_Resume.pdf" onClick="ga('send', 'event', 'PDF', 'Download_Resume', 'Download_Resume','1');" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-download fa-stack-1x fa-inverse" ></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="experience">
        <div class="my-auto">
          <h2 class="mb-5">Experience</h2>
		  <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Sr Data Engineer<br>Data Engineer</h3>
              <div class="subheading mb-5">Peterbilt Motors Company (Paccar Inc.), Denton, TX</div>
              <p align="justify">Key Responsibilities:<br>
								 Data Engineering focuses on making possible fast, accurate, and reliable access to data. I build data pipelines, manage a data warehouse, and support the production use of our data. I advocate for good data practices and make sure that our business users are able to make good data driven decisions.
								<p align="justify">
								Primary Duties:<br>
									•	Provide engineering on modern, cloud-based data processing technology stack<br>
									•	Build data pipelines, data validation frameworks, job schedules with emphasis on automation and scale<br>
									•	Contribute to overall architecture, framework, and design patterns to store and process high data volumes<br>
									•	Ensure product and technical features are delivered to spec and on-time<br>
									•	Design and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology<br>
									•	Proactively support product health by building solutions that are automated, scalable, and sustainable be relentlessly focused on minimizing defects and technical debt<br>
									•	Focus on making possible fast, accurate, and reliable access to data.<br>
									•	Interact with Peterbilt engineers, analysts and data scientists to gather the requirements.<br>
									•	Create the build requirement document by translating the end users requirements into developable features.<br>
									•	Build the flowchart, architecture designs of the code pipelines based on the requirements using the lucid charts.<br>
									•	Based on the architecture design, build pipeline in Amazon Web Services (AWS) Cloud in combination of Python, Java and Shell Scripting.<br>
									•	Unit and Integration Test case document is create for the Services to be implemented.<br>
									•	Perform the Unit testing of each service implemented in AWS cloud based on the test cases document.<br>
									•	Perform Integration testing to test the entire pipeline of services.<br>
									•	Also create a Cloud formation/ Terraform template to migrate the code pipeline for Development environment to Test and Production environment.<br>
									•	As part of deployment AWS EC2 (Amazon Elastic Compute Cloud) instance, EKS (Amazon Elastic Container Service for Kubernets) instance will be created.<br>
									•	Involve in creating Amazon Lambda functions to run quick serverless functionalities.<br>
									•	Build the CloudWatch event to monitor the production pipeline performance and various matrix.<br>
									•	Write the SQL queries, Stored Procedure and Function in Snowflake to pull the data based on users requirements.<br>
									•	Involve in Data Migration from OnPrem systems which includes SQL Database, Teradata, Excel files to Cloud service like Snowflake and Amazon S3.<br>
									•	Build the Attunity Tasks and Snowflake streams to migrate the data to cloud.<br>
									•	Also involve in creating Tableau dashboards to visualize the data for the Quick Win presentation.<br>
									•	Knowledge of Agile / Scrum methodology as a development process.<br>
									•	Involve in Sprint Planning, Review and Retro, Daily standup and Weekly meetings to keep the Ongoing and Upcoming task on track. <br></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Dec 2021- Present <br>Nov 2019 - Dec 2021</span>
            </div>
          </div>
		  <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Business Systems Analyst</h3>
              <div class="subheading mb-5">Netrush, Vancouver, WA</div>
              <p align="justify">Key Responsibilities:<br>
								As a Business Systems Analyst, I am responsible for bridging the business needs of reporting high volumes of business data and insights into scalable, impactful tools (including the Netrush Brand Portal). Also work with the advisory and research teams to design data methods for capturing desired data, analyzing the captured data and creating methods for reporting findings to stakeholders. As part of process I Interact with users, stakeholders, Brand Managers, Marketing Analysts to collect User Requirements and translates into User Stores, Wireframes and Acceptance Criteria for development of the company wide Tools/Features.
								<br>
								As a Business Intelligence Developer, I am responsible for both ad hoc and organization wide data projects. Involving in writing SQL Quires and Procedures to extracts reports from Netrush database and Developing Power BI reports and dashboards to transfer insights in data to visualizations. I build Data Model using star schema for Analytics and Reporting purpose. As a day to day responsivities, I involve in reviewing reports and dashboard for fine tuning performance using Query Optimization techniques, applying reporting best practices. I develop ETL pipelines and activities in Azure Data Factory, to extract data from Netrush database and blob storage and apply transformation to convert production data into dimensional data and load into the reporting database. Also, I schedule these pipelines using Azure triggers to load SCD data and on daily intervals. In Azure Automation, I write Runbook scripts to Auto scale PowerBI premium tiers and schedule the Power BI reports refresh to keep upto date data at reporting server, also implementing incremental refresh of reports, trigger dashboards via email to stakeholders.</p>
								<p align="justify">
								Primary Duties:<br>
									•	Develop Azure ETL Pipeline/ Activities to extract data from production database/ Azure storage, transform into dimensional data and load into reporting database.<br>
									•	Schedule these pipeline using Azure Automation. <br>
									•	Writing SQL Quires and Procedures to extracts reports from SQL server database.<br>
									•	Developing Power BI reports and dashboards to transfer insights in data to visuals.<br>
									•	Implement calculation/formula to improve  reporting performance.<br>
									•	Dimensional Data Modeling using star schema.<br>
									•	Review reports and dashboards for fine tuning performance using Query Optimization techniques, applying reporting best practices.<br>
									•	Scheduling report refresh at keep current data and Implementing Incremental refresh to reduce refresh time. <br>
									•	Trigger reports to different end users via email.<br>
									•	Writing Runbook scripts in Azure Portal to Auto scaling Power BI preimium Tier on peek hrs.<br>
									•	Interacting with users, stakeholders, Brand Managers, Marketing Analysts to collect User Requirements.<br>
									•	Writing Features/Tool/Product Backlog Items with Description, User Stories, Acceptance Criteria.<br>
									•	Involving in Standup, Sprint planning, User Review, Release and retro meetings.<br>
									•	Creating Mockups/Wireframes as per user requirements.<br>
									•	Testing the Tools/Reports/Dashboards and Creating Bugs if required.<br>
									•	Creating Report, Tool release documents as a guidance for End Users and Stakeholders.<br>
									•	Familiar with Azure DevOps/ VSTS for Agile process.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Oct 2018 - Nov 2019</span>
            </div>
          </div>
		  <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Graduate Assistant</h3>
              <div class="subheading mb-3">Athletics Department, Texas Tech University, Lubbock,TX</div>
              <p align="justify">In this position, I developed and maintained the Texas Tech Athletic Department website. Created the tableau dashboards and stories to get insights and trends of expenditures and athletes’ performance in each game and also in academics. We created different kinds of charts, reports and motion charts using tableau. Deployed these dashboards to the Tableau public and used tableau bridge to keep the data UpToDate at the server side. To perform all these operations strong knowledge of database was required for writing SQL queries, PL/SQL procedures and functions to get data from various data sources. Experienced in logical and physical Data Modeling using normalizing Techniques, creating Tables, Views, Constraints, Index and Triggers using SQL and PL/SQL. Performed pivoting, clustering, created calculated and parameter fields to model the data according to the business needs. Tableau server was scheduled to send reports to via email on different time. 
			  <br><br>
			  Also provided IT and desktop support for the athletics staff. As part of my duties, I created group policies, managed file shares and printers within the organizational unit. I also participated in the upkeep of the inventory by creating a tool to find missing equipment. Last, I participate in the deployment of software and patches through KACE and in the deployment of computer images through MDT via PXE boot.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Dec 2016 - May 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Software Engineer S2- Data Warehouse Analyst</h3>
              <div class="subheading mb-3">EVRY India Pvt Ltd, Bangalore, India</div>
              <p align="justify">As a Software Engineer, I was responsible for the Extraction, Transformation and Loading of data from multiple sources using Informatica ETL tools and SQL. To perform these duties, a deep understanding of the Data Warehousing concepts were required. Thus, I used Informatica Designer for designing and developing mappings from varied transformation logic using Expression, Source Qualifier, Filter, Router, Joiner, Lookup, Sorter, Aggregator and Update Strategy. I also designed ETL logic for Dimension Tables using SCD – Type1 and Type2. Last, I worked with heterogeneous data sources like DB2, Flat Files and Mainframe Files to accomplish the goals of the organization.
			  <!--<br><br>
			  I was also responsible for designing and developing multi-tier applications using core Java and web service APIs using Spring, Hibernate, REST, Maven frameworks and application servers like Apache Tomcat. Worked on different modules of Spring Application Framework (Spring MVC, Spring Core, Spring Boot, Spring JDBC) . Experienced in Database Design, Creation and Management of Schemas, writing SQL queries, PL/SQL procedures and functions to get data from various data sources and version controlling tools like Git. As a testing practice used JUnit for building Unit Test Cases and involved in peer code reviews. I also performed production deployments and post production support and written support documents. --></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Aug 2014 - Aug 2016</span>
            </div>
          </div>

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
	  
        <div class="my-auto">
          <h2 class="mb-5">Education</h2>
		  
		   <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Master of Science in Computer Science</h3>
              <div class="subheading mb-3">Texas Tech University,United States</div>
              <!--<p>GPA: 3.42</p>-->
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">August 2016 - May 2018</span>
            </div>
          </div>
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Bachelor of Engineering in Computer Science</h3>
              <div class="subheading mb-3">Visvesvaraya Technological University,India</div>
              <!--<p>GPA: 3.6</p>-->
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">August 2011 - June 2014</span>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Diploma in Computer Science</h3>
              <div class="subheading mb-3">R N Shetty Polytechnic,India</div>
              <!--<p>GPA: 3.5</p>-->
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2008 - June 2011</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="skills">
        <div class="my-auto">
          <h2 class="mb-5">Skills</h2>

          <div class="subheading mb-3">Programming Languages</div>
          <ul class="list-inline list-icons">
			<link rel="stylesheet" href="https://cdn.rawgit.com/konpa/devicon/df6431e323547add1b4cf45992913f15286456d3/devicon.min.css">
            <li class="list-inline-item">
              <i class="devicon-python-plain-wordmark"></i>
            </li>
			<li class="list-inline-item">
              <i class="devicon-java-plain-wordmark"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicons-javascript"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicon-c-plain-wordmark"></i>
            </li>
            <li class="list-inline-item">
              <i class="devicons devicon-mysql-plain-wordmark"></i>
            </li>
            <li class="list-inline-item">
              <i class="subheading mb-3">PL/SQL</i>
            </li>
            <li class="list-inline-item">
              <i class="subheading mb-3">Shell scripting</i>
            </li>
          </ul>

          <div class="subheading mb-3">Web Designing Skills</div>
          <ul class="fa-ul mb-0">
			
			<li>
              <i class="fa-li fa fa-check"></i>
              HTML5, HTML, CSS, JavaScript, PHP, XML, JSON</li>
            <li>
          </ul>
		  <div class="subheading mb-3">Database</div>
          <ul class="fa-ul mb-0">
		  
            <li>
              <i class="fa-li fa fa-check"></i>
              Oracle 9i, SQL Server</li>
            <li>
          </ul>
		  <div class="subheading mb-3">Cloud Experience</div>
          <ul class="fa-ul mb-0">
		  
            <li>
              <i class="fa-li fa fa-check"></i>
              <b>Microsoft Azure : </b>Data Factory, Azure Storage, Azure Automation,Runbooks</li>
            <li>
			<li>
              <i class="fa-li fa fa-check"></i>
              <b>Amazon Web Services :</b> S3, Lambda, SQS, SES, SNS, Step functions, Cloud Front, SSO,Cognito, EC2, EKS, ECR, Glue Etc.., 
			</li>
			<li>
              <i class="fa-li fa fa-check"></i>
              <b>Other : </b>Snowflake, Docker, Spark, Jenkins, Terraform, Gremlin, Git Actions
			</li>
            <li>
          </ul>
		  <div class="subheading mb-3">Business Intelligence Tools</div>
          <ul class="fa-ul mb-0">
		  
            <li>
              <i class="fa-li fa fa-check"></i>
             Tableau, Tableau Desktop, Tableau Bridge, Tableau Server and Tableau Public, Power BI Pro, Power BI Desktop, Google Analytics, Looker, QuickBase, D3.js</li>
            <li>
          </ul>
		  <div class="subheading mb-3">Frameworks</div>
          <ul class="fa-ul mb-0">
		  
            <li>
              <i class="fa-li fa fa-check"></i>
              Spring, Hibernate, Spring Boot (MVC)</li>
            <li>
          </ul>
		  
		  <div class="subheading mb-3">Tools & Utilities</div>
          <ul class="fa-ul mb-0">
		  
            <li>
              <i class="fa-li fa fa-check"></i>
              Attunity, Apache Spark, Informatica PowerCenter Client 9.5.1, Informatica Data Explorer (IDE),  Informatica Cloud, SQL Developer and Toad, Power Exchange</li>
            <li>
          </ul>
        </div>
		
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Offline no shortage priority Analysis (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">Management spends time using EWI data to lookup defects and shortages on trucks to create priority lists to guide the offline flow to park trucks, the process of creating these lists is time consuming and the lists are not up to date by the time they reach the operators.<br>
			  
			  <b>*</b> I built a dashboard that will help the management to quickly see the number of defects on the trucks.<br>
			  <b>*</b> Group and drill down to the trucks by part shortage category, line location , defect owner<br>
			  <b>*</b> Identify and Prioritize low defect trucks and work on those so that those are ready to deliver <br>
			  <b>*</b> See the truck parking location in the lot so that drivers can bring it to the plant for the rework.<br>
			  <b>*</b> As this dashboard is real time which eliminates the manual work which is done before every shift.<br>
			  </p>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Materials Parts Summary Analysis (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">This analysis was for the materials team to identify the number of trucks with part shortages by build day, and allows the plant to adjust build rates to reduce the impact of part shortages.<br>
			  
			  Key KPI were:<br>
			  <b>*</b> Ability to see the Net on hand parts quantity which includes on hand and shipped quantity.<br>
			  <b>*</b> Ability to see part utilization for the next 30 business days.<br>
			  <b>*</b> Visibility to identity the part shortage using utilization calculated using build rate, required parts quantity based on scheduled build date/ chassis line order number<br>
			  <b>*</b> Ability to see the shortage across all the manufacturing plants<br>
			  <b>*</b> Ability to see chassis that are affected by part shortage, drill through part family<br>
			  </p>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Truck Traffic Analysis (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">Project Overview & Objectives
			  Dealer Development needs to implement the connected truck data into various analyses. Using the connected truck data, Dealer Development can accurately understand the level of truck activity in a given dealership location’s territory. This insight produces more precise recommendations for the need of more (or possibly less) of the following in a given dealership’s area/market:<br>
			  <b>*</b> Number of dealerships<br>
			  <b>*</b> Number of Service Bays<br>
			  <b>*</b> Hours of Parts & Service operation<br>
			  <b>*</b> Number of Mobile Service units<br>
			  <b>*</b> Number of Outside Parts Salespeople<br>
			  <br>
			  In this Project I worked as a Data engineer and performed below duties:<br><br></li>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>I conducted the POC to prove that processing millions of remote diagnostics data in python and finding the respected dealers using the user provided json territory files.<br></li>
			  <li><i class="fa-li fa fa-check"></i>I developed the architecture to run the dealercd assignments on aws environment and using snowflake remote diagnostics and dealer territory files and store the results back to snowflake. And to implement business requirements.<br></li>
			  <li><i class="fa-li fa fa-check"></i>I also did the cost estimation of the project which mainly includes AWS Services cost and Snowflake compute and storage cost.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed terraform templates for deploying the aws infrastructure on dev, test and prod accounts. Which involves AWS EC2, S3 bucket, Lambda function, SQS, SNS email<br></li>
			  <li><i class="fa-li fa fa-check"></i>Performance of the process was a key part of the project so used python multiprocessing based on EC2 instance cores and daemon thread to sync logs to s3 while executing the script.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed Jenkins pipeline for deploying the infrastructure. And pipeline to build docker containers using the code from VSTS repo and deploy an EC2 instance and also pipeline to execute docker containers when file is uploaded to s3 and also on cron schedule.<br></li>
			  </p>
            </div>
          </div>
		  
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Engine Reliability Analysis (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">Project Overview & Objectives
			  The Engine Reliability Analysis (ERA) project was initiated as a directive from PACCAR's executive team. The MX13 EPA17 engine family is experiencing multiple catastrophic failure modes which have resulted in a significant increase in warranty costs. These issues also have a profound impact on the customer downtime, and the customer's perception of PACCAR's quality. <br>
			  
			  The ERA project is assisting with two primary use cases:<br>
			  1.) Relative Risk of Failure - For each failure mode and each affected chassis, provide a projected probability of failure over the period of interest (e.g. next 12 months). Ranking by this probability provides a relative risk that allows the business to prioritize resources (dealership capacity, parts, warranty dollars, etc)<br><br>
			  2.) Warranty Forecast - Projecting the number of failures that will occur within the warranty period. These projections are used to set the accrual amount for PACCAR's financial reporting. In addition to standard reliability projections, simulations are used to estimate the impact of the planned field interventions (campaigns, customer education, etc).<br><br>
			  3.) Root Cause Analysis - Provide engineering insights with variables that are highly correlated with reliability to assist with generating root cause hypothesis. It is important to note that the analytics findings provide correlated variables, but these are not necessarily causal factors.<br><br>
			  In this Project I worked as a Data engineer and performed below duties:<br><br></li>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>Designed the architecture running ML model on AWS using services like S3, SQS, SNS, EC2, ECR, Lambda and data from snowflake and parquet files<br></li>
			  <li><i class="fa-li fa fa-check"></i>Designed snowflake er diagram for storing ML data, artifacts, build logs, database views.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Created the snowflake views using multiple tables/views from multiple databases for pulling the chassis, ops, EASOP and cab database and summarizing the remote diagnostics records for latest miles, engine revolution and engine hours. <br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed the terraform templates for deploying the aws infrastructure on dev, test and prod accounts.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed multiple Jenkins pipeline for deploying infrastructure, pipeline for deploying ML code and pipeline for running the models when file placed on S3 bucket.<br></li>
			  </p>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Warranty Repairs Per one Hundred (R100) (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">
			  Responsibilities:<br><br></li>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>I wrote a complex snowflake queries and views to calculate R100 for NIS/3MIS, Vehicle and Engine meeting at ATA and Parts level using Warranty, OPS and CAB database for the calculation R100 = total failures /  total parts used.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Python scripts are written to read data from snowflake db and calculate R100 and store the results into project specific schemas.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Designed an architecture for running a script on schedule using AWS services (EC2, SNS, CloudWatch Event, S3, ECR)<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed terraform template for deploying the AWS Infrastructure.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed Jenkins pipeline for running the deployments and running the scripts on schedule using cron job.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Used VSTS as a code repository and Jira for project management.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed a tableau dashboard for visualizing the R100 at ATA and Parts level, used various features of tableaus like charts, filters, actions, show hide menu, cross charts filtering, drill through.<br></li>
			  </p>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Failure Mode Identification Project (Peterbilt)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">I handled the data engineering apart of this project<br>
			  Responsibilities:<br><br></li>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>Setting up the snowflake schemas, tables and views<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed architecture for deploying the pipelines<br></li>
			  <li><i class="fa-li fa fa-check"></i>Wrote python data util scripts for data scientists for reading and writing data to snowflake.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed flask application: <br>
			  <b>*</b> for downloading the claims and tagged failure mode for warranty meeting <br>
			  <b>*</b> for uploading validated files to snowflake by performing validation check <br>
			  <b>*</b> uploading and downloading Top ATA/Parts status files, Golden Set, Training data.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Deployed flask application on AWS EC2 instance using docker container.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Sync flask application log files to AWS S3 using APScheduler package.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Developed terraform template for deploying AWS infrastructure<br></li>
			  <li><i class="fa-li fa fa-check"></i>Used VSTS as code repository and Jira for project management<br></li>
			  </p>
            </div>
          </div>
		  
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Netrush Platform </h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">This is a Netrush in-house retail domain project, focused on developing e-commerce platform. I am responsible for gathering user requirement and translating into development features. Also involve in testing tools.</p>
            </div>
          </div>
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Netrush Portal </h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">This is a Netrush Analytical and reporting project, focused on developing reports and dashboards to get insights of sales, inventory, compliance, violations, pricing etc.,.</p>
            </div>
          </div>
		  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">National Eligibility Database (NEDB Carrier Loads)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">This is Insurance domain project for the client HMS (Healthcare Management System). We receive files on timely bases from Commercial Insurance carriers, Third party, Pharmacy Benefit Managers, Government Payers for which we do ETL (Extract Load and Transformation) and perform Data match using proprietary logic in order to Cost Avoidance and Recovery.</p>
			  <p align="justify">
			  I was responsible for the Extraction, Transformation and Loading of data from multiple sources using Informatica ETL tools and SQL. To perform these duties, a deep understanding of the Data Warehousing concepts were required. Thus, I used Informatica Designer for designing and developing mappings from varied transformation logic using Expression, Source Qualifier, Filter, Router, Joiner, Lookup, Sorter, Aggregator and Update Strategy. I also designed ETL logic for Dimension Tables using SCD – Type1 and Type2. Last, I worked with heterogeneous data sources like DB2, Flat Files and Mainframe Files to accomplish the goals of the organization.</p>
			  
			  Team Activities:<br>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>Preparing Minutes Of Meeting<br></li>
			  <li><i class="fa-li fa fa-check"></i>Giving internal training on Informatica PowerCenter.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Preparing Informatica training materials.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Updating the Track.</li></ul>
			  <br><br>				
            </div>
          </div>
		  
		  <h2 class="mb-5">Academic Projects</h2>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Geospatial Visualization of Twitter Data</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p align="justify">
			  This is an National Science Foundation funded research, it is focused on building a model for the researcher to collect tweets using Twitter API and analyze the trends in Twitter across Texas counties using hashtags to find the topics people are discussing every day. As hashtags are metadata, they help to recognize topic discussion at a first glance. Analyzing the evolution of topics over time shows how a discussion increases or fades away when new topics emerge.  Discussions can also be suddenly switched to new topics.  Human discussion behaviors can be studied through this type of analysis of topic evolution. Part of this analysis is frequency distribution of the hashtags which is calculated to highlight the use of the hashtags for each day. Another part of this analysis is applying topic modeling algorithms to tweets to bring out the generalized view point of the tweets. Over all tweets, conclusions are difficult to derive as many tweets consist of diverse topics; thus, topic analysis methods applied on a smaller set of tweets pertaining to hashtags shows more uniformity of thought in the tweets. These topics and hashtags are mapped to their respective county on a daily and weekly basis to find the co-occurring and identical discussion across the state of Texas. Further, to know the opinion of tweeters on these topics, tweets related to the hashtags are extracted and the sentiment, positive or negative, of each tweet is calculated using supervised machine learning algorithms. These positive and negative sentiment tweets are mapped to their respective county to find tweeter perspectives on the topics at each county level; thus, enabling researchers to factor in tweet analysis to their geospatial research of disease indicators, governmental policy issues, and social norms.
			  <br> <br>
			  Application was built in Python using different packages like for UI {wxPython}, for charts and reports {bokeh}, for Analysis {pandas, SciKit-Learn, NumPy, SciPy, Plotly, NLTK, Gensim, Tweepy,json,urllib etc.}.</p>
			  
			  
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Movie Scope</h3>
              <div class="subheading mb-3"></div>
              <p>Abstract</p>
			  <p align="justify">
			  We implement a system through which we can analyze and predict the box office collection of the movies. We build the system in such a way that people from various backgrounds can also interact with the system and the data we have. Through the interaction they can also find hidden trends and insights which some of us miss. The data contains various variables which are compiled from various sources. The data can be extremely exhaustive but we have tried to collect the information essential for the analysis of trends and insights.</p>
			  <p>Introduction</p>
			  <p align="justify">
			  Each year there are hundreds of movies which are released and being produced. The revenue of the movies depends on various factors like genre of the movie, critics review, quality of trailer, stardom of lead actor, studio of the movie being produced, director of the movie and various people included in movie making process. It can also depend on the current state of the economy, spending trends and habit of people, stock market and various other economic factors. The movie industry was worth more than 100 billion dollars in 2016[1]. And it is only expected to grow with number of new movies and stories and also the growing population. So it becomes useful to predict how much amount of money will one movie generate based on historic figures. In this paper we take into amount all the factors described above like genre, critics review, lead actor, studio etc.
			  
			  This demo helps to analyze and visualize the trends of the box office collection. End user can visualize the collections over the years based one the MPAA (Motion Picture Association of America) Rating's. Classify based on years, title name. Word cloud of top two movies help the user to know more about the movie summery, cast, genres. Review sentiment helps to know people opinion about the movie.</p>
			  <a href="https://arunjegarkal.github.io/Movie-Scope-Tableau/index.html" target="_blank">Click Here for Dashboard</a>
			  <br>
			  <a href="https://github.com/Arunjegarkal/Movie-Scope-Tableau" target="_blank">Click Here for Github</a>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">A High Performance, Scalable Event-Bus System for Distributed Data Synchronization (Java Application)</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p align="justify">
			  In this project, we basically focus to overcome the limitations of the existing techniques and our primary goal is to provide the support for fully transactional communication between the clients and the eventbus thereby making the communication more effective.
			  
			  Websockets replaces the native socket and multithreading concepts of java, which is the efficient way of establishing the pub-sub connection. Serialized the message delivery between publisher-eventbus-subscriber by using ‘Avro’ which is one of the serialization techniques used in hadoop.</p>
			  
			  <a href="https://github.com/Arunjegarkal/Scalable_Event_Bus" target="_blank">Click Here for Github</a>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Gnutella style P2P file sharing system (Java Application)</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p  align="justify">
			  This project is about implementation of Gnutella-style peer-to-peer (P2P) file sharing system, where signal peer act as both client and server with performing the file search within the specified network without using the Indexing server. I have implemented using java socket programming and multithreading concepts. When peer is started its respected server thread is started which runs in the background, given config file is read to find its peer and starts the client thread for each neighbor and when the neighboring peer gets online connection is established this connection is used to search file among the network. Server thread is used to search file within its local directory and store the result into the list once the local search is performed it reads the config file to start the client thread to all its neighbors. time-to-live (TTL) is attached to the message along with the filename need to be searched. TTL is used for preventing query messages from being forwarded infinitely many times. Client thread is used to send the file to search to servers which are connected to it. Server Download thread performs the downloading of file from the peer which is selected by the user. Same server and client threads are used for broadcasting the file modification message to all the neighboring peers which has that modified file. </p>
			  
			  <a href="https://github.com/Arunjegarkal/Gnutella-style-peer-to-peer-P2P-file-sharing-system" target="_blank">Click Here for Github</a>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Napster style peer-to-peer file sharing system(Java Application)</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p  align="justify">
			  I have used Socket Implementation of Napster-style peer-to-peer (P2P) file sharing system. The main idea of this project is to Create the indexing server which stores all the file details and bringing it online so that all the peers connect to it and register its files on the server. Once files are registered it provided with 2 option to search for the file in other peer or can exit the execution, if search is selected file name is asked and port number and sent to the server, sever will return the peer id’s which as the respected file and then client has to select the peer id from which file to be downloaded. Downloaded file will be stored into the requested peer’s shared directory. Every Peer can provide its shared director and can register its files so that other peer’s can search and download them.
			  
			  Every Client Server connection is done using Sockets, given the port number and the Server name client establishes a connection with the Server registering files and searching files among peers.</p>
			  
			  <a href="https://github.com/Arunjegarkal/Gnutella-style-peer-to-peer-P2P-file-sharing-system" target="_blank">Click Here for Github</a>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Visualizing time series data</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p  align="justify">
			  Representing the unemployment information for different states in USA based on the data set available Bureau of Labor and Statistics . This website allows the user to select the date range to generate a time series visualization. </p>
			  
			  <p>Functionalities</p>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>User can zoom into time interval to get more insight.<br></li>
			  <li><i class="fa-li fa fa-check"></i>User can visualize time series based on States.<br></li>
			  <li><i class="fa-li fa fa-check"></i>User can select multiple states from the map for visualization.<br></li>
			  <li><i class="fa-li fa fa-check"></i>User can compare unemplyment rate between all the states.</li>
			  <li><i class="fa-li fa fa-check"></i>User can unselecte the selected states from the map. <br></li>
			  <li><i class="fa-li fa fa-check"></i>User can reset the visualization.<br></li></ul>
			  <br>			  
			  <a href="https://github.com/Arunjegarkal/Visualization" target="_blank">Click Here for Github</a>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Virtual PC Viewer (Android Application)</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p  align="justify">
			  Virtual PC Viewer is a remote control system which allows the user to view and interact with one computer (“remote desktop machine”) to cellular phones (“viewer”) using wireless internet connection. A viewer is provided on the cellular phone that enables the user to see and manipulate the desktop of various remote systems such as Windows operating system. The system to be accessed must be running a Virtual PC Viewer server and it must be attached to a network.	</p>
			  <br>
			  <p>Description</p>
			  <p  align="justify">
			  Virtual PC Viewer is a client server mobile application and is an ultra-thin client system based on a simple display protocol that is platform independent. This software allows to communicate with the remote system and can have control on the peripheral devices like key board, mouse from the cellular phones. The remote system’s administration tasks can also be performed from the cellular phone. Popular uses for this technology include remote technical support and accessing files on one's work computer to the mobile device. Here the mobile device will act as client and remote machine is called server The user of Virtual PC Viewer has to register at client machine before connecting to server. Registered user can connect to the remote desktop machine through IP address and PORT number of remote desktop machine. After successful connection user can access remote desktop machine. Android native code is used to make client-server communication and better User Interface is provided to make the application user friendly.</p>
			  
			  <p>Application</p>
			  <p  align="justify">
			  One of the primary application of this project is being able to access, operate a computer remotely from the mobile as if remote machine is in your hand, User can access virtual files from remote machine to mobile device.</p>
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Doodle Blink (Android Mobile Game)</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p  align="justify">
			  Now days the ease for playing software games has increased tremendously. Due to busy schedule and nuclear families people op to play software games rather than playing manually. Especially those which are quickly accessible via computers or mobiles. This game is one among such games which is fun filled, probabilistic in nature. The Doodle Blink is a skating game, the doodle will always be blinking, and instead the user needs to jump Doodle to skip the obstacles. When game gets over will get score in score board and it display only highest score.</p>
			  <br>
			  <p>Description</p>
			  <p  align="justify">
			  The Project “Doodle Blink” is a test of accuracy, strength and concentration. This is developed platform HTML5 and with the support of Phonegap. Doodle Blink game is the task completion game in which the doodle (person) will be always blinking, the doodle will be on the skating board will be with the doodle till the end of the task competition, for each of the task completion there will be the restricted time, the task contains the obstacles etc. Doodle blink a contiguous game in which doodle is always blinking and Instead the user needs to jump doodle to skip the obstacles and in the first level obstacles are less and speed is also less and we have to score 1000 points then will go for second level in this obstacles are more and speed is also more and for going the third level we have score 3000 points and in this level we have collect the positive points. In score board consists of highest top 4 score will be displayed.</p>
			  
			  <br><br>
            </div>
          </div>
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Know before you buy (Web Application)</h3>
              <div class="subheading mb-3"></div>
              <p  align="justify">It is a website suggests user the products (Laptop or its accessories) from different brands based on his specifications (cost, brand, model). This website gives the complete idea regarding the latest products in the market.</p>
			  <p>
			  
			  <br><br>				
            </div>
          </div>
		  
		  
		  <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">File Converter (Java Application)</h3>
              <div class="subheading mb-3"></div>
              <p align="justify">PDF is global standard for capturing and reviewing rich information. It is more secure and dynamic than ever. Basically PDF files are read only , which are not editable so we need to convert this file to some other format which provides more editing options one of the format is Microsoft word.</p>
			  <p align="justify">
			  File Converter is a Windows system program. It supports 98/2000/XP/2007/2008 platform this software is developed on Java platform using Itext jar, This allows user to convert PDF file to document format file and vice versa and also PDF to TXT format. This software has the ability to retain the original layouts of the source file in the conversion.</p>
			  <p>Application:</p>
			  <ul class="fa-ul mb-0">
			  <li><i class="fa-li fa fa-check"></i>Image extraction from PDF file.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Concatenation of two PDF files.<br></li>
			  <li><i class="fa-li fa fa-check"></i>There is no problem in image processing.<br></li>
			  <li><i class="fa-li fa fa-check"></i>Splitting of one PDF file to two PDF files.</li>
			  <li><i class="fa-li fa fa-check"></i>Converting PDF file to Text file.</li></ul>
			  <br><br>				
            </div>
          </div>
		  
		<div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">QuesZen</h3>
              <div class="subheading mb-3"></div>
              <p>Introduction</p>
			  <p align="justify">
			 In Education Centers the most required task is preparing question papers for internal, mid terms and final exams. Which need to meet in time and most secretive. Even university papers which are printed and sent to respective colleges have a chance of leak at last movement.</p>
			  <br>
			  <p>Description</p>
			  <p align="justify">
			  QuesZen is developed on platform VB.net for frontend and MS Access as backend application and crystal report for generating the question papers into document format. The Software that generates the question paper by picking the questions dynamically from Question bank Database at the specified level of difficulty papers will be more secretive and it will be in time. It provides 3 levels of paper generation i.e. easy, medium and difficult levels. It allows selecting subject, level number of sections and number of questions of specified marks, it will generate different question set of each press of generate button by picking random question from the database and also allows you to change particular question from the generated paper, and provide you option save the 3 sets paper for each exam type.</p>
			  
			  <br><br>
            </div>
          </div>
		
      </section>
	  <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="certifications">
        <div class="my-auto">
          <h2 class="mb-5">Certifications </h2>
          <ul class="fa-ul mb-0">
			<li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Tableau 10</U><span><img src="img/Tableau.PNG" width='500' height='350'></span></a> </p>
			</li>
            <li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Google Analytics</U><span><img src="img/Google_Analytics.PNG" width='500' height='350'></span></a> </p>
			</li>
            <li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Apache Spark</U><span><img src="img/apache_spark.PNG" width='500' height='350'></span></a> </p>
			</li>
			<li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Looker First Look</U><span><img src="img/Looker.PNG" width='500' height='350'></span></a> </p>
			</li>
			<li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Oracle Database 12c: Advanced SQL</U><span><img src="img/Oracle_adv.PNG" width='500' height='350'></span></a> </p>
			</li>
			<li>
              <i class="fa-li fa fa-certificate text-warning"></i>
				<p><a class="popup" href="#"><U>Oracle Database 12c: Basic SQL</U><span><img src="img/Oracle.PNG" width='500' height='350'></span></a> </p>
			</li>
            
          </ul>
        </div>
      </section>
      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="awards">
        <div class="my-auto">
          <h2 class="mb-5">Honors, Awards &amp; Achievements </h2>
          <ul class="fa-ul mb-0">
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              Geospatial Visualization of Twitter Data - Poster presentation at national workshop for NSF grant support.</li>
			  
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              Awarded competitive scholarship: 75% fee waiver by Texas Tech University.</li>
            <li>
              <i class="fa-li fa fa-trophy text-warning"></i>
              WebMaster at Texas Tech India student Association.</li>
            
          </ul>
        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
